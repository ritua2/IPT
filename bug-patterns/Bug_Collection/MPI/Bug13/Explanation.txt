Summary: The buggy version of this example was erroneous at multiple levels. It seems like the algorithm was not approproately captured in the code. Process 0 updates half of the elements of a matrix and sends those values to process 1, and process 1 computes the other half and sends those values to Process 0. In the program, the ordering of sends and receives was incorrect - each send should have a mathcing receive but in the program, both the processes were trying to send the messages to each other without waiting to receive. Secondly, the message tags were not used correctly. The fixed version solves the problem nut can be optimized further by using a collective operation. Alternatively, MPI_Sendrecv calls could have been used.



``````````````````````````````


* Error message

```
Fatal error in MPI_Send: Other MPI error, error stack:
MPI_Send(186): MPI_Send(buf=0x604638, count=1, MPI_DOUBLE, dest=1, tag=2, MPI_COMM_WORLD) failed
MPID_Send(93): DEADLOCK: attempting to send a message to the local process without a prior matching receive
aborting job:
Fatal error in MPI_Send: Other MPI error, error stack:
MPI_Send(186): MPI_Send(buf=0x604638, count=1, MPI_DOUBLE, dest=1, tag=2, MPI_COMM_WORLD) failed
MPID_Send(93): DEADLOCK: attempting to send a message to the local process without a prior matching receive
Rank 0 [Wed Jun 19 11:09:58 2019] [c0-0c0s3n2] Fatal error in MPI_Send: Other MPI error, error stack:
MPI_Send(186): MPI_Send(buf=0x604728, count=1, MPI_DOUBLE, dest=0, tag=1, MPI_COMM_WORLD) failed
MPID_Send(93): DEADLOCK: attempting to send a message to the local process without a prior matching receive
aborting job:
Fatal error in MPI_Send: Other MPI error, error stack:
MPI_Send(186): MPI_Send(buf=0x604728, count=1, MPI_DOUBLE, dest=0, tag=1, MPI_COMM_WORLD) failed
MPID_Send(93): DEADLOCK: attempting to send a message to the local process without a prior matching receive
```
