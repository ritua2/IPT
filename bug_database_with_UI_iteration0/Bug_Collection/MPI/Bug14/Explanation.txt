Node 0 performs both Send to itself and Receive from itself. 
This may create deadlock. 

``````````````````````````````
	for (i = 0; i < numprocs; i++) //masternode distributes matrix A to every single core
	{   
		...
		MPI_Send(&meta[0], 3, MPI_INT, i, TAG, MPI_COMM_WORLD);
		...
		MPI_Send(&MA[0], ASpalten*AZeilen, MPI_DOUBLE, i, TAG, MPI_COMM_WORLD);
		...
		MPI_Send(&MB[0], ASpalten*BSpalten, MPI_DOUBLE, i, TAG, MPI_COMM_WORLD);
		...
	}
``````````````````````````````
Since the node needs to block for the send call and wait for the receive, this may create deadlock. 


* Error message

```
Fatal error in MPI_Send: Other MPI error, error stack:
MPI_Send(186): MPI_Send(buf=0x62d530, count=3, MPI_INT, dest=0, tag=0, MPI_COMM_WORLD) failed
MPID_Send(93): DEADLOCK: attempting to send a message to the local process without a prior matching receive
aborting job:
Fatal error in MPI_Send: Other MPI error, error stack:
MPI_Send(186): MPI_Send(buf=0x62d530, count=3, MPI_INT, dest=0, tag=0, MPI_COMM_WORLD) failed
MPID_Send(93): DEADLOCK: attempting to send a message to the local process without a prior matching receive
```

```
This error was fixed by changing:

 for (i = 1; i < numprocs; i++) //masternode distributes matrix A to every single core
--->       
 for (i = 0; i < numprocs; i++) //masternode distributes matrix A to every single core

```
