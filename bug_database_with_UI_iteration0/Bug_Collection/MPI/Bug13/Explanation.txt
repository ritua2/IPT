MPI_Send may or may not block. 
It will block until the sender can reuse the sender buffer.
In details, in the buggy version:
``````````````````````````````
	if(process_id==0)
    {
		for(i=1;i<=5;i++)
        {
            for(j=1;j<=Ny;j++)
            {
				...
                MPI_Send(&a[Nxl-1][j],1,MPI_DOUBLE,0,1,MPI_COMM_WORLD);
            }
        }
        for(i=1;i<=5;i++)
        {
            for(j=1;j<=Ny;j++)
            {
            	...
                MPI_Recv(&a[1][j],1,MPI_DOUBLE,1,1,MPI_COMM_WORLD,&status);
                ...
            }
        }
	}
	...
	if(process_id==1)
    {
        for(i=6;i<=10;i++)
        {
            for(j=1;j<Ny;j++)
            {
               ...
                MPI_Send(&a[2][j],1,MPI_DOUBLE,1,2,MPI_COMM_WORLD);
            }
        }
        for(i=6;i<=10;i++)
        {
            for(j=1;j<Ny;j++)
            {
                MPI_Recv(&a[Nxl][j],1,MPI_DOUBLE,0,1,MPI_COMM_WORLD,&status);
                ...
            }
        }
    }

``````````````````````````````
Some implementations will return to the caller when the buffer has been sent to a lower communication layer. 
Some others will return to the caller when there's a matching MPI_Recv() at the other end. 
So it's up to the MPI implementation whether if this program will deadlock or not.
(Stackoverflow explaination)